{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sentiment Analysis",
   "id": "e697ac59c8453f7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T22:35:28.721964Z",
     "start_time": "2024-06-05T22:35:28.223173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "from os import chdir\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src import TEST_DATA_PATH, TRAIN_DATA_PATH, CACHE_DIR\n",
    "from src.preprocessing.DataCleaner import DataCleaner\n",
    "from src.preprocessing.Stemmer import Stemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# import libraries\n",
    "import string\n",
    "import sys\n",
    "from gc import collect\n",
    "from os import chdir\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src import TEST_DATA_PATH, TRAIN_DATA_PATH\n",
    "from src.preprocessing.DataCleaner import DataCleaner\n",
    "from src.preprocessing.Stemmer import Stemmer\n"
   ],
   "id": "c3afcfd164e35298",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/krystian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T22:35:28.729167Z",
     "start_time": "2024-06-05T22:35:28.724652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_FEATURES = 200_000\n",
    "RANDOM_SEED = 42\n",
    "SAMPLE_SIZE = 0.005"
   ],
   "id": "4c159e2e468eb166",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T22:35:28.737423Z",
     "start_time": "2024-06-05T22:35:28.731305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# change directory to root\n",
    "chdir('..')"
   ],
   "id": "52110ce0a4371960",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load & Prepare Data",
   "id": "a7d22dd307db25e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T22:35:49.061126Z",
     "start_time": "2024-06-05T22:35:28.740439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# loading data, we will use only review and label columns, and skip description column with index 1\n",
    "train_data = pd.read_csv(TRAIN_DATA_PATH, names=['label', 'review'], usecols=[0, 2])\n",
    "test_data = pd.read_csv(TEST_DATA_PATH, names=['label', 'review'], usecols=[0, 2])\n",
    "print(f'Train data shape: {train_data.shape}')\n",
    "print(f'Test data shape: {test_data.shape}')"
   ],
   "id": "5d07ab199329da63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (3000000, 2)\n",
      "Test data shape: (650000, 2)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T22:35:50.360593Z",
     "start_time": "2024-06-05T22:35:49.063462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# drop neutral labels\n",
    "train_data = train_data[train_data['label'] != 3]\n",
    "test_data = test_data[test_data['label'] != 3]\n",
    "\n",
    "# change the labels to 0 for negative and 1 for positive\n",
    "train_data['label'] = train_data['label'].apply(lambda x: 0 if x < 3 else 1)\n",
    "test_data['label'] = test_data['label'].apply(lambda x: 0 if x < 3 else 1)"
   ],
   "id": "130aedaacd66bc12",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sample Data",
   "id": "e6ef46a62a03425f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T22:35:52.523893Z",
     "start_time": "2024-06-05T22:35:50.361725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# sample the data to speed up the experiments\n",
    "# make sure to stratify the data to keep the same distribution of labels\n",
    "train_data, _ = train_test_split(train_data, test_size=(1 - SAMPLE_SIZE), stratify=train_data['label'],\n",
    "                                 random_state=RANDOM_SEED)\n",
    "test_data, _ = train_test_split(test_data, test_size=(1 - SAMPLE_SIZE), stratify=test_data['label'],\n",
    "                                random_state=RANDOM_SEED)"
   ],
   "id": "c590d2f9f0b25709",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preprocessing Pipeline\n",
    "\n",
    "The pipeline will consist of the following steps:\n",
    "\n",
    "1. Data Cleaning:\n",
    "    - Convert all words to lowercase.\n",
    "    - Remove stopwords.\n",
    "    - Remove punctuation.\n",
    "    - Remove URLs.\n",
    "    - Remove handles (e.g., Twitter handles).\n",
    "    - Remove emojis.\n",
    "    - Remove extra spaces.\n",
    "\n",
    "2. Stemming:\n",
    "    - Reduce words to their root form using a stemming algorithm.\n",
    "\n",
    "3. Vectorization:\n",
    "    - Convert text into a matrix of token counts.\n",
    "    - Set the ngram_range parameter to (1, 2) to include both individual words and pairs of consecutive words.\n",
    "    - Use a predefined constant MAX_FEATURES to limit the number of most frequent words, discarding less frequent words.\n",
    "\n",
    "4. TF-IDF Transformation:\n",
    "    - Transform the matrix of token counts into a normalized TF-IDF representation.\n",
    "    - This step reduces the importance of frequently occurring words and increases the importance of rarely occurring words, which could be more informative."
   ],
   "id": "896b54514db3f927"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T22:35:52.545544Z",
     "start_time": "2024-06-05T22:35:52.540457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# In jupyter notebook, we will use only part of the data for more efficient data analysis and models comparison.\n",
    "# Split the data into train and test sets\n",
    "X_train = train_data['review']\n",
    "y_train = train_data['label']\n",
    "X_test = test_data['review']\n",
    "y_test = test_data['label']"
   ],
   "id": "b44741772197494d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T22:35:52.562172Z",
     "start_time": "2024-06-05T22:35:52.547529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('cleaner', DataCleaner()),\n",
    "    ('stemmer', Stemmer()),\n",
    "    ('vectorizer', CountVectorizer(ngram_range=((1, 2)), max_features=MAX_FEATURES)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "],\n",
    "    verbose=True,\n",
    "    memory=CACHE_DIR\n",
    ")"
   ],
   "id": "5ec527efd6a1d1cf",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T22:36:37.578736Z",
     "start_time": "2024-06-05T22:36:14.770989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# preprocess the data\n",
    "X_train_preprocessed = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessing_pipeline.transform(X_test)"
   ],
   "id": "7f4066566ff2139c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........... (step 1 of 4) Processing cleaner, total=   6.8s\n",
      "[Pipeline] ........... (step 2 of 4) Processing stemmer, total=   7.5s\n",
      "[Pipeline] ........ (step 3 of 4) Processing vectorizer, total=   1.7s\n",
      "[Pipeline] ............. (step 4 of 4) Processing tfidf, total=   0.1s\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compare different models",
   "id": "cef852da2f587f72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def score_model(model):\n",
    "    y_pred = model.predict(X_test_preprocessed)\n",
    "    precision_score = metrics.precision_score(y_test, y_pred)\n",
    "    recall_score = metrics.recall_score(y_test, y_pred)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    try:\n",
    "        roc_auc_score = metrics.roc_auc_score(y_test, model.decision_function(X_test_preprocessed))\n",
    "    except:\n",
    "        roc_auc_score = metrics.roc_auc_score(y_test, model.predict_proba(X_test_preprocessed)[:, 1])\n",
    "    return precision_score, recall_score, f1_score, accuracy, roc_auc_score"
   ],
   "id": "d6e47bec6bcbb804"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_confusion_matrix(model, model_name, accuracy, ax):\n",
    "    y_pred = model.predict(X_test_preprocessed)\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum()\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', ax=ax, cmap='Blues', cbar=False)\n",
    "    ax.set_title(f\"{model_name}\\nAccuracy: {accuracy:.3f}\")\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n"
   ],
   "id": "9f128a949d441f34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# prepare a dictionary of classifiers\n",
    "classifiers = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'SVM': SVC(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}"
   ],
   "id": "5961dfdd5c047ff6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# prepare a dictionary of hyperparameters for each classifier\n",
    "param_distributions = {\n",
    "    'Naive Bayes': {},\n",
    "    'SVM': {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf', 'poly', 'sigmoid']},\n",
    "    'Logistic Regression': {'C': [0.1, 1, 10, 100], 'penalty': ['l1', 'l2', 'elasticnet', 'none']},\n",
    "    'Random Forest': {'n_estimators': [10, 50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]},\n",
    "    'Gradient Boosting': {'n_estimators': [10, 50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001], 'max_depth': [3, 10, 20]}\n",
    "}"
   ],
   "id": "f26ae3b72773f495"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = pd.DataFrame(columns=['Classifier', 'Precision', 'Recall', 'F1 Score', 'Accuracy', 'ROC AUC Score'])\n",
    "\n",
    "for classifier_name, classifier in classifiers.items():\n",
    "    random_search = RandomizedSearchCV(classifier, param_distributions=param_distributions[classifier_name], n_iter=10, cv=5, n_jobs=-1)\n",
    "    random_search.fit(X_train_preprocessed, y_train)\n",
    "    \n",
    "    precision, recall, f1, accuracy, roc_auc = score_model(random_search.best_estimator_)\n",
    "    results = results.append({\n",
    "        'Classifier': classifier_name,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Accuracy': accuracy,\n",
    "        'ROC AUC Score': roc_auc\n",
    "    }, ignore_index=True)"
   ],
   "id": "3be38baeb8e73de7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tworzenie nowego obiektu figure i axes dla wykresów\n",
    "fig, axes = plt.subplots(nrows=len(classifiers), figsize=(5, 5 * len(classifiers)))\n",
    "\n",
    "# Przeprowadzenie losowego przeszukiwania hiperparametrów dla każdego klasyfikatora\n",
    "for ax, (classifier_name, classifier) in zip(axes, classifiers.items()):\n",
    "    random_search = RandomizedSearchCV(classifier, param_distributions=param_distributions[classifier_name], n_iter=10, cv=5, n_jobs=-1)\n",
    "    random_search.fit(X_train_preprocessed, y_train)\n",
    "    \n",
    "    # Obliczanie dokładności dla najlepszego modelu\n",
    "    accuracy = metrics.accuracy_score(y_test, random_search.predict(X_test_preprocessed))\n",
    "    \n",
    "    # Rysowanie macierzy pomyłek\n",
    "    plot_confusion_matrix(random_search.best_estimator_, classifier_name, accuracy, ax)\n",
    "\n",
    "# Wyświetlanie wykresów\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "60d3fb17ec3aa37a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
